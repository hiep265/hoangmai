# from configs.config_system import SYSTEM_CONFIG
# from icecream import ic

######## TEST ELASTICSEARCH ########
# from source.retriever.elastic import ElasticQueryEngine
# from source.retriever.extract_specifications import extract_info

# query = "camera k√≠nh hi·ªÉn vi"
# demands = extract_info(query=query)
# # print(demands)
# response = ElasticQueryEngine(member_code="NORMAL").search_db(demands=demands)
# print(response[0])

# from source.retriever.elastic_search.elastic_helper import RetrieveHelper
# es = RetrieveHelper()
# es.check_specific_field("sold_quantity")
####### TEST CHAT SEASION ########
# from source.generate.chat_seasion import chat_interface
# while True:
#     query = input("Nh·∫≠p c√¢u h·ªèi: ")
#     if query == "exits":
#         break
#     response = chat_interface(query=query)
#     ic(response)

######### TEST CRAWLER ########
# from utils.crawler.crawler_website import CrawlerWebsites
# crawler = CrawlerWebsite(url="https://www.dienmayxanh.com/may-lanh")
# df = crawler.get_info_product()

######### TEST ROUTER ########
# from source.router.router import decision_search_type
# query = ""
# type = decision_search_type(query=query)
# print("Type: ", type['content'])

# ######### TEST TOOL SEARCH ########
# from source.similar_product.searcher import SimilarProductSearchEngine
# from langchain_core.prompts import PromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from source.prompt.template import PROMPT_SIMILAR_PRODUCT

# engine = SimilarProductSearchEngine()
# similar_product_found = engine.search(product_find=type.split("|")[1].strip())
# product_found_str = "\n".join(similar_product_found)


# prompt = PromptTemplate(
#     input_variables=['question', 
#                      'context'],
#     template=PROMPT_SIMILAR_PRODUCT
# )
# model = SYSTEM_CONFIG.load_rag_model()
# chain = prompt | model | StrOutputParser()
# response = chain.invoke({'question': query, 
#                         'context': product_found_str})
# print(response)

######### TEST CHAT CLS PRODUCT ########
# from source.router.router import classify_product
# query = "ƒëi·ªÅu h√≤a gi√° 10 tri·ªáu"
# response = classify_product(query=query)
# print(response)

######### TEST CHAT API CALL ########
# from source.generate.chat_seesion import Pipeline
# from handle_request import handle_request

# response = handle_request(
#     # InputText = "c√≥ lo·∫°i n√†o kh√°c n·ªØa kh√¥ng",
#     # InputText = "cho t√¥i xem m√°y h√†n",
#     InputText = "cho t√¥i xem tua v√≠t",
#     UserName="Nguy·ªÖn C√¥ng Vinh",
#     IdRequest="8386",
#     PhoneNumber='0123456789',
#     Address='H√† N·ªôi',
#     MemberCode="NORMAL",
#     NameBot=None)
# print(response)
#so s√°ch chatbot d√πng backend l√† code python v√† n8n

# from utils.user_helper import UserHelper
# UserHelper().save_conversation(phone_number="0123456789", id_request="123", query="gmsdgsdm", response="sdmn")
# response = UserHelper().load_conversation(conv_user="0123456789", id_request="123")
# print(response)

######### TEST CHATCHIT  ############
# from langchain_core.prompts import PromptTemplate
# from source.prompt.template import PROMPT_CHATCHIT

# while True:
#     question = input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")
#     if question == "q":
#         break
#     template = PromptTemplate(
#                     input_variables=['question'],
#                     template=PROMPT_CHATCHIT).format(question=question)
#     response = SYSTEM_CONFIG.load_chatchit_model().invoke(template).content
#     print(response)

######### TEST DATABASE ############
# from utils.postgre_logger import PostgreHandler
# postgres_handle = PostgreHandler()
# postgres_handle.create_table()
# postgres_handle.connector.close()

# import logging
# from utils.postgre_logger import PostgreHandler

# if __name__ == "__main__":
#     logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

#     postgres_handle = PostgreHandler()

#     if postgres_handle.connector:
#         # N·∫øu c·∫ßn t·∫°o l·∫°i table (t√πy b·∫°n)
#         # postgres_handle.create_table()
        
#         postgres_handle.connector.close()
#         logging.info("Connection closed.")
#     else:
#         logging.error("Failed to connect to PostgreSQL.")


######### TIEN XU LY DU LIEU ##########
# import pandas as pd
# from bs4 import BeautifulSoup
# import re

# # ƒê·ªçc file Excel
# df = pd.read_excel('product_update.xlsx')

# # Ch·ªâ gi·ªØ l·∫°i c√°c h√†ng c√≥ gi√° tr·ªã 'kich_hoat' l√† 1
# df = df[df['kich_hoat'] == 1]

# # B·ªè ti·ªÅn t·ªë "Thi·∫øt b·ªã>>" v√† "Linh ki·ªán>>" trong c·ªôt 'danh_muc'
# df['danh_muc'] = df['danh_muc'].str.replace('^Thi·∫øt b·ªã>>', '', regex=True)
# df['danh_muc'] = df['danh_muc'].str.replace('^Linh ki·ªán>>', '', regex=True)

# # Thay 'KH√ÅC' b·∫±ng chu·ªói r·ªóng trong c·ªôt 'thuong_hieu'
# df['thuong_hieu'] = df['thuong_hieu'].replace('KH√ÅC', '')

# # X√≥a c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt (n·∫øu t·ªìn t·∫°i)
# columns_to_drop = [
#     'gia_cu','hinh_anh','link_video','xoa_anh','trong_luong','kich_hoat','sp_moi','main_keyword',
#     'tom_tat','seo_title','seo_keyword','seo_description','hien_trang_chu','khuyen_mai'
# ]
# df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)

# # H√†m l√†m s·∫°ch HTML, icon v√† x√≥a ƒëo·∫°n Ho√†ng Mai Mobile
# def clean_html(text):
#     if pd.isna(text):
#         return ''
#     soup = BeautifulSoup(text, 'html.parser')
#     plain_text = soup.get_text(separator=' ', strip=True)

#     hoangmai_pattern = (
#         r"(N·∫øu c√≥ b·∫•t k·ª≥ th·∫Øc m·∫Øc n√†o li√™n quan t·ªõi s·∫£n ph·∫©m, h√£y li√™n h·ªá v·ªõi Ho√†ng Mai Mobile ƒë·ªÉ ƒë∆∞·ª£c chƒÉm s√≥c t·ªët nh·∫•t nh√©!\s*)?"
#         r"HO√ÄNG MAI MOBILE Chuy√™n cung c·∫•p: Linh ki·ªán ƒëi·ªán tho·∫°i, d·ª•ng c·ª• s·ª≠a ch·ªØa, v·∫≠t t∆∞ √©p k√≠nh Smartphone\s*"
#         r"ƒê·ªãa ch·ªâ: Ng√µ 117 Th√°i H√† - Ph∆∞·ªùng Trung Li·ªát - Qu·∫≠n ƒê·ªëng ƒêa - TP H√† N·ªôi\s*"
#         r"(‚òé\s*)?ƒêi·ªán tho·∫°i: 098\.215\.3333"
#     )
#     plain_text = re.sub(hoangmai_pattern, '', plain_text)

#     cleaned_text = re.sub(r'[‚úî‚úÖ‚û§‚ñ∫‚òÖ‚òÜ‚Ä¢+‚òéüîßüõ†Ô∏è‚ùåüí°‚û°Ô∏è‚Üí‚Üê‚Üë‚Üì‚ö°üåüüî•üì±üîç‚ú®üîπüì¢üìåüîäüîÑüõ°üîãüì°üè†üå°üîã]', '', plain_text)
#     cleaned_text = re.sub(r'\s{2,}', ' ', cleaned_text).strip()
#     return cleaned_text

# # L√†m s·∫°ch c·ªôt 'mo_ta'
# df['mo_ta'] = df['mo_ta'].apply(clean_html)

# # S·∫Øp x·∫øp theo c·ªôt 'danh_muc'
# df.sort_values(by='danh_muc', inplace=True)

# # ƒê·ªïi t√™n c√°c c·ªôt
# df.rename(columns={
#     'id': 'product_info_id',
#     'ma_san_pham': 'product_code',
#     'ten_san_pham': 'product_name',
#     'gia_ban': 'lifecare_price',
#     'danh_muc': 'group_product_name',
#     'thuong_hieu': 'trademark',
#     'ton_kho': 'inventory',
#     'mo_ta': 'specifications',
#     'anh_avatar': 'avatar_images',
# }, inplace=True)
# df['group_product_name'] = df['group_product_name'].str.lower()
# df['group_name'] = df['product_name']
# # Ghi l·∫°i v√†o file m·ªõi
# df.to_excel('data_final_NORMAL_merged.xlsx', index=False)



# import pandas as pd

# # ƒê·ªçc file Excel
# df1 = pd.read_excel('Book1.xlsx')  # ch·ª©a 'M√£ SP' v√† 'M√¥ t·∫£'
# df2 = pd.read_excel('product-export-12-05-2025.xlsx')  # ch·ª©a 'product_code' v√† 'specifications'

# # T·∫°o t·ª´ ƒëi·ªÉn mapping t·ª´ M√£ SP -> M√¥ t·∫£
# desc_map = dict(zip(df1['M√£ SP'], df1['M√¥ t·∫£ cho s·∫£n ph·∫©m']))

# # C·∫≠p nh·∫≠t specifications n·∫øu c√≥ m√¥ t·∫£ m·ªõi trong excel1
# df2['mo_ta'] = df2['ma_san_pham'].map(lambda x: desc_map.get(x, df2.loc[df2['ma_san_pham'] == x, 'mo_ta'].values[0]))

# # Xu·∫•t file k·∫øt qu·∫£
# df2.to_excel('product_update.xlsx', index=False)


# import pandas as pd

# # ƒê·ªçc file Excel
# file_path = 'Dulieu Hao.xlsx'  # thay b·∫±ng ƒë∆∞·ªùng d·∫´n t·ªõi file c·ªßa b·∫°n
# df = pd.read_excel(file_path)

# # ƒê·ªïi t√™n c√°c c·ªôt
# df.rename(columns={
#     'ID': 'product_code',
#     'name': 'product_name',
#     'category': 'group_product_name',
#     'amount': 'lifecare_price',
#     'provider': 'trademark',
#     'stock': 'inventory',
#     'description': 'specifications',
#     'image': 'avatar_images',
#     'link': 'link_product'
# }, inplace=True)

# # S·∫Øp x·∫øp theo group_product_name
# df.sort_values(by='group_product_name', inplace=True)
# df['group_product_name'] = df['group_product_name'].str.lower()
# df['group_name'] = df['product_name']

# df.to_excel('data_final_NORMAL_merged.xlsx', index=False)







# import pandas as pd

# # ƒê·ªçc file Excel
# file_path = 'data_final_NORMAL_merged.xlsx'  # thay b·∫±ng ƒë∆∞·ªùng d·∫´n file th·∫≠t c·ªßa anh
# df = pd.read_excel(file_path)

# # L·ªçc nh·ªØng d√≤ng m√† group_product_name ch·ª©a t·ª´ "v·ªâ" (kh√¥ng ph√¢n bi·ªát ch·ªØ hoa/th∆∞·ªùng)
# df_vi = df[df['group_product_name'].str.lower().str.contains('v·ªâ', na=False)]

# # Ho·∫∑c l∆∞u ra file m·ªõi n·∫øu c·∫ßn
# df_vi.to_excel('data_vi_lam_chan.xlsx', index=False)





# import pandas as pd

# file_path = 'data_final_NORMAL_merged.xlsx'
# df = pd.read_excel(file_path)

# def clean_product_name(name):
#     if isinstance(name, str) and '_' in name:
#         # L·∫•y ph·∫ßn sau d·∫•u g·∫°ch d∆∞·ªõi ƒë·∫ßu ti√™n
#         return name.split('_', 1)[1].strip()
#     return name

# df['product_name'] = df['product_name'].apply(clean_product_name)

# df['group_name'] = df['product_name']

# df.to_excel('data_productname_cleaned_v2.xlsx', index=False)




# import pandas as pd
# import requests
# from bs4 import BeautifulSoup
# import logging

# logging.basicConfig(level=logging.INFO)

# def _get_jpg_image_link(url: str) -> str:
#     try:
#         response = requests.get(url, timeout=5)
#         soup = BeautifulSoup(response.text, "html.parser")
#         image_tags = soup.find_all("img")
#         for img in image_tags:
#             src = img.get("src")
#             if src and ".jpg" in src:
#                 if src.startswith("http"):
#                     return src
#                 else:
#                     return "https://hoangmaimobile.vn" + src
#     except Exception as e:
#         logging.error("CRAWL IMAGE ERROR: " + str(e))
#     return ""

# # ƒê·ªçc file Excel
# file_path = "data_final_NORMAL_merged1.xlsx"
# df = pd.read_excel(file_path)

# # Ki·ªÉm tra c·ªôt avatar_images c√≥ t·ªìn t·∫°i kh√¥ng
# if 'avatar_images' in df.columns:
#     df['avatar_images'] = df['avatar_images'].apply(lambda url: _get_jpg_image_link(url) if isinstance(url, str) and url.endswith(".html") else url)
# else:
#     logging.error("Kh√¥ng t√¨m th·∫•y c·ªôt 'avatar_images' trong file Excel.")

# # Ghi l·∫°i k·∫øt qu·∫£ ra file m·ªõi
# df.to_excel("data_final_NORMAL_merged.xlsx", index=False)
